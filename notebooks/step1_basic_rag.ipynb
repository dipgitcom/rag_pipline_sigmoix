{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fa113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import openai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21335199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "openai.api_key = \"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\n",
    "\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% PDF Processing Class\n",
    "class PDFProcessor:\n",
    "    \"\"\"Extract and process text from PDF financial documents\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract all text from PDF file\"\"\"\n",
    "        print(f\"Extracting text from: {pdf_path}\")\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text = \"\"\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                page_text = page.get_text()\n",
    "                text += f\"\\n--- Page {page_num + 1} ---\\n{page_text}\"\n",
    "            \n",
    "            doc.close()\n",
    "            print(f\"Successfully extracted {len(text):,} characters from {len(doc)} pages\")\n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting PDF: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize extracted text\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove special characters but preserve financial notation\n",
    "        text = re.sub(r'[^\\w\\s\\$\\%\\.\\,\\(\\)\\-\\+\\:]', ' ', text)\n",
    "        \n",
    "        # Remove page headers/footers patterns\n",
    "        text = re.sub(r'--- Page \\d+ ---', '', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Split text into semantic chunks with metadata\"\"\"\n",
    "        clean_text = self.clean_text(text)\n",
    "        chunks = self.text_splitter.split_text(clean_text)\n",
    "        \n",
    "        processed_chunks = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            processed_chunks.append({\n",
    "                \"content\": chunk,\n",
    "                \"chunk_id\": i,\n",
    "                \"length\": len(chunk),\n",
    "                \"word_count\": len(chunk.split())\n",
    "            })\n",
    "        \n",
    "        print(f\"Created {len(processed_chunks)} text chunks\")\n",
    "        return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310998e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Vector Retrieval Class\n",
    "class VectorRetriever:\n",
    "    \"\"\"Handle vector embedding and similarity search\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        print(f\"Loading embedding model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        \n",
    "    def build_index(self, chunks: List[Dict]):\n",
    "        \"\"\"Build FAISS vector index from text chunks\"\"\"\n",
    "        self.chunks = chunks\n",
    "        texts = [chunk[\"content\"] for chunk in chunks]\n",
    "        \n",
    "        print(\"Generating embeddings...\")\n",
    "        # Generate embeddings with progress bar\n",
    "        embeddings = self.model.encode(\n",
    "            texts, \n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Generated embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f95eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index for cosine similarity\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f197bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"FAISS index built with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Retrieve most relevant chunks for query\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
    "        \n",
    "        print(f\"Searching for: '{query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b932a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode query\n",
    "        query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search index\n",
    "        scores, indices = self.index.search(\n",
    "            query_embedding.astype('float32'), \n",
    "            top_k\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e18abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid result\n",
    "                results.append({\n",
    "                    \"content\": self.chunks[idx][\"content\"],\n",
    "                    \"similarity_score\": float(score),\n",
    "                    \"chunk_id\": self.chunks[idx][\"chunk_id\"],\n",
    "                    \"length\": self.chunks[idx][\"length\"]\n",
    "                })\n",
    "        \n",
    "        print(f\"Retrieved {len(results)} relevant chunks\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Answer Generation Class\n",
    "class FinancialQAGenerator:\n",
    "    \"\"\"Generate answers using retrieved context\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.model = model\n",
    "        \n",
    "    def generate_answer(self, query: str, context_chunks: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate answer using GPT with financial context\"\"\"\n",
    "        \n",
    "        # Combine retrieved contexts\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Context {i+1}: {chunk['content']}\" \n",
    "            for i, chunk in enumerate(context_chunks)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb123fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt for financial QA\n",
    "        prompt = f\"\"\"You are a financial analyst assistant. Based on the following context from Meta's financial reports, provide a precise and factual answer to the query.\n",
    "\n",
    "CONTEXT FROM FINANCIAL DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Provide direct, factual answers based ONLY on the context provided\n",
    "- Include specific numbers, percentages, and dollar amounts when available\n",
    "- If the information is not in the context, clearly state this\n",
    "- Keep the answer concise but comprehensive\n",
    "- Cite specific figures when making claims\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\", \n",
    "                        \"content\": \"You are a financial analyst providing accurate information from financial reports.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                max_tokens=500,\n",
    "                temperature=0.1,\n",
    "                presence_penalty=0.1\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"model_used\": self.model,\n",
    "                \"tokens_used\": response.usage.total_tokens,\n",
    "                \"context_chunks_used\": len(context_chunks)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error generating response: {str(e)}\",\n",
    "                \"model_used\": self.model,\n",
    "                \"tokens_used\": 0,\n",
    "                \"context_chunks_used\": len(context_chunks)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1194528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Main Pipeline Execution\n",
    "\n",
    "def run_basic_rag_pipeline():\n",
    "    \"\"\"Execute the complete basic RAG pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: BASIC RAG PIPELINE FOR FINANCIAL DOCUMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # File path - update with your PDF location\n",
    "    pdf_path = \"meta_q1_2024.pdf\"  # Update this path\n",
    "    \n",
    "    # Initialize components\n",
    "    print(\"\\n1. Initializing components...\")\n",
    "    processor = PDFProcessor()\n",
    "    retriever = VectorRetriever()\n",
    "    generator = FinancialQAGenerator()\n",
    "    \n",
    "    # Process PDF document\n",
    "    print(\"\\n2. Processing PDF document...\")\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"ERROR: PDF file not found at {pdf_path}\")\n",
    "        print(\"Please download Meta's Q1 2024 earnings report and update the path\")\n",
    "        return\n",
    "    \n",
    "    raw_text = processor.extract_text_from_pdf(pdf_path)\n",
    "    if not raw_text:\n",
    "        print(\"ERROR: Failed to extract text from PDF\")\n",
    "        return\n",
    "    \n",
    "    chunks = processor.chunk_text(raw_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903618d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector index\n",
    "    print(\"\\n3. Building vector search index...\")\n",
    "    retriever.build_index(chunks)\n",
    "    \n",
    "    # Test queries for Step 1\n",
    "    test_queries = [\n",
    "        \"What was Meta's revenue in Q1 2024?\",\n",
    "        \"What were the key financial highlights for Meta in Q1 2024?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n4. Testing RAG pipeline with queries...\")\n",
    "    results = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"QUERY: {query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Retrieve relevant context\n",
    "        relevant_chunks = retriever.retrieve(query, top_k=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c617f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display retrieved chunks\n",
    "        print(\"\\nRETRIEVED CONTEXT:\")\n",
    "        for i, chunk in enumerate(relevant_chunks, 1):\n",
    "            print(f\"\\nChunk {i} (Similarity: {chunk['similarity_score']:.3f}):\")\n",
    "            print(f\"{chunk['content'][:200]}...\")\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"\\nGENERATING ANSWER...\")\n",
    "        answer_result = generator.generate_answer(query, relevant_chunks)\n",
    "        \n",
    "        print(f\"\\nFINAL ANSWER:\")\n",
    "        print(f\"{answer_result['answer']}\")\n",
    "        \n",
    "        print(f\"\\nMETADATA:\")\n",
    "        print(f\"- Tokens used: {answer_result['tokens_used']}\")\n",
    "        print(f\"- Context chunks: {answer_result['context_chunks_used']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "        results[query] = {\n",
    "            \"answer\": answer_result['answer'],\n",
    "            \"retrieved_chunks\": relevant_chunks,\n",
    "            \"metadata\": {\n",
    "                \"tokens_used\": answer_result['tokens_used'],\n",
    "                \"chunks_used\": len(relevant_chunks),\n",
    "                \"avg_similarity\": np.mean([c['similarity_score'] for c in relevant_chunks])\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "    print(\"\\n5. Saving results...\")\n",
    "    output_file = \"step1_basic_rag_results.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11debe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Document processed: {pdf_path}\")\n",
    "    print(f\"Text chunks created: {len(chunks)}\")\n",
    "    print(f\"Queries processed: {len(test_queries)}\")\n",
    "    print(f\"Average retrieval similarity: {np.mean([r['metadata']['avg_similarity'] for r in results.values()]):.3f}\")\n",
    "    print(f\"Total tokens used: {sum([r['metadata']['tokens_used'] for r in results.values()])}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Execute Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete pipeline\n",
    "    results = run_basic_rag_pipeline()\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for query, result in results.items():\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"Answer: {result['answer'][:150]}...\")\n",
    "        print(f\"Confidence: {result['metadata']['avg_similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Evaluation and Analysis\n",
    "def evaluate_step1_results(results: Dict):\n",
    "    \"\"\"Basic evaluation of Step 1 results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1 EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to evaluate\n",
    "    metrics = {\n",
    "        \"avg_retrieval_score\": np.mean([\n",
    "            r['metadata']['avg_similarity'] for r in results.values()\n",
    "        ]),\n",
    "        \"avg_chunks_per_query\": np.mean([\n",
    "            r['metadata']['chunks_used'] for r in results.values()\n",
    "        ]),\n",
    "        \"total_tokens\": sum([\n",
    "            r['metadata']['tokens_used'] for r in results.values()\n",
    "        ]),\n",
    "        \"queries_processed\": len(results)\n",
    "    }\n",
    "    \n",
    "    print(\"Quantitative Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ecda26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative assessment\n",
    "    print(\"\\nQualitative Assessment:\")\n",
    "    for query, result in results.items():\n",
    "        answer = result['answer']\n",
    "        \n",
    "        # Check for key indicators of good answers\n",
    "        has_numbers = bool(re.search(r'\\$[\\d,]+|\\d+%|\\d+\\.\\d+', answer))\n",
    "        mentions_meta = 'meta' in answer.lower()\n",
    "        mentions_q1_2024 = any(term in answer.lower() for term in ['q1 2024', 'first quarter 2024'])\n",
    "        \n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"  Contains numbers: {has_numbers}\")\n",
    "        print(f\"  Mentions Meta: {mentions_meta}\")\n",
    "        print(f\"  References Q1 2024: {mentions_q1_2024}\")\n",
    "        print(f\"  Answer length: {len(answer)} characters\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation if results exist\n",
    "if 'results' in globals():\n",
    "    evaluation_metrics = evaluate_step1_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
